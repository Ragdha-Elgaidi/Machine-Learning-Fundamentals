# Activation Function Properties
## There are a wide variety of activation functions that we can use. Activation functions should be:
   - Nonlinear
   - Differentiable -- preferably everywhere
   - Monotonic
   - Close to the identity function at the origin
- We can loosen these restrictions slightly. For example, ReLU is not differentiable at the origin. Others, like monotonicity, are very important and cannot be reasonably relaxed.

 ![activation-functions](https://github.com/Ragdha-Elgaidi/Machine-Learning-Fundamentals/assets/76912120/13b8dc2c-bfc8-47eb-bc35-28c0937a5be5)
